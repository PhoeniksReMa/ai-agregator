services:
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
#    command:
#      - ollama pull qwen2.5:3b-instruct-q5_K_M
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - NVIDIA_VISIBLE_DEVICES=${GPU_MISTRAL}
    volumes:
      - ${OLLAMA_MODELS}:/root/.ollama
    ports: ["11434:11434"]
    runtime: nvidia

#  xtts:
#    build: ./xtts
#    restart: unless-stopped
#    environment:
#      - COQUI_TOS_AGREED=1
#      - HF_HUB_ENABLE_HF_TRANSFER=0
#      - HF_HUB_DISABLE_TELEMETRY=1
#      - NVIDIA_VISIBLE_DEVICES=${GPU_TTS}
#      - TTS_HOME=/data/tts
#    volumes:
#      - ${XTTS_CACHE}:/data/tts
#    ports: ["8021:8021"]
#    runtime: nvidia

#  whisper:
#    build: ./whisper
#    restart: unless-stopped
#    environment:
#      - WHISPER_CACHE=${WHISPER_CACHE}
#      - HF_HUB_ENABLE_HF_TRANSFER=0
#      - HF_HUB_DISABLE_TELEMETRY=1
#      - NVIDIA_VISIBLE_DEVICES=${GPU_WHISPER}
#    volumes:
#      - ${WHISPER_CACHE}:/root/.cache
#    ports: ["8022:8022"]
#    runtime: nvidia

  comfyui:
    build: ./comfyui
    restart: unless-stopped
    environment:
      - CLI_ARGS=--listen 0.0.0.0 --port 8188
      - NVIDIA_VISIBLE_DEVICES=${GPU_SD}
    ports: ["8188:8188"]
    volumes:
      - ${COMFY_MODELS}:/opt/ComfyUI/models
    runtime: nvidia

  gateway:
    build:
      context: .
      dockerfile: gateway/Dockerfile
    working_dir: /app
    restart: unless-stopped
    ports: ["8080:8080"]
    env_file:
      - .env
    depends_on: [ollama, xtts, whisper, comfyui]
